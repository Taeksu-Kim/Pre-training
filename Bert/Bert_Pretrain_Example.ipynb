{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8FcYRa48Gc",
        "outputId": "2993541c-a859-401f-9261-b5d0daa78925"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 27.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertForMaskedLM, pipeline\n",
        "from transformers.utils import logging\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer"
      ],
      "metadata": {
        "id": "p_Xle8ykcS4B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYqha-sZ4KAS"
      },
      "source": [
        "!mkdir my_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z0pedAT44Gl",
        "outputId": "36c6ca2a-fc0f-4fc2-8ef8-30b3c1578b5f"
      },
      "source": [
        "# 전체적인 동작 확인을 위한 작은 데이터셋\n",
        "\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt\n",
        "\n",
        "file=\"./my_data/wiki_20190620_small.txt\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "awk: cannot open ./cookie (No such file or directory)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1323k  100 1323k    0     0  2575k      0 --:--:-- --:--:-- --:--:--  270M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq_xSrYBKFuv"
      },
      "source": [
        "# 큰 데이터셋\n",
        "\n",
        "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n",
        "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o my_data/wiki_20190620.txt\n",
        "\n",
        "# file=\"./my_data/wiki_20190620.txt\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIEUFdT4WIqd"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFTgG7yv4_n7"
      },
      "source": [
        "!mkdir wordPieceTokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGFiNRpV5Ban",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097ec028-1a4c-4df2-d028-0b934115618a"
      },
      "source": [
        "# Initialize an empty tokenizer\n",
        "wp_tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,   # \" \", \"\\t\", \"\\n\", \"\\r\" 등의 공백 문자는 Token으로 하지 않고 제거. [\"좋은\",\" \",\"예제\"] -> [\"좋은\",\"예제\"]\n",
        "    handle_chinese_chars=True,  # 한자는 모두 char 단위로 분할\n",
        "    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n",
        "    lowercase=False,    # Hello -> hello\n",
        ")\n",
        "\n",
        "wp_tokenizer.train(\n",
        "    files=file,\n",
        "    vocab_size=20000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "    wordpieces_prefix=\"##\"\n",
        ")\n",
        "\n",
        "wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wordPieceTokenizer/my_tokenizer-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngd6Z3aJ57VR",
        "outputId": "7595c762-2687-49fb-8130-62f1ff17878c"
      },
      "source": [
        "print(wp_tokenizer.get_vocab_size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_max_input_len = 512"
      ],
      "metadata": {
        "id": "o1FUBq3aeITZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wtm3umi6geG"
      },
      "source": [
        "tokenizer = BertTokenizerFast(\n",
        "    vocab_file='./wordPieceTokenizer/my_tokenizer-vocab.txt',\n",
        "    max_len=model_max_input_len,\n",
        "    do_lower_case=False,\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvjurG9_bWND",
        "outputId": "298a4102-9c8c-43db-c378-73c07c03ce73"
      },
      "source": [
        "print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[UNK]', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpmBH_F5XYxf",
        "outputId": "2041222d-3b5f-4b73-9bf0-597925bf6db4"
      },
      "source": [
        "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
        "print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLE2ZQEp5GBq"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ka2mjYRuKzo"
      },
      "source": [
        "logger = logging.get_logger(__name__)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ekdXKzlt2qO"
      },
      "source": [
        "class TextDatasetForNextSentencePrediction(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        short_seq_probability=0.1,\n",
        "        nsp_probability=0.5,\n",
        "    ):\n",
        "        # caching\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
        "        self.short_seq_probability = short_seq_probability\n",
        "        self.nsp_probability = nsp_probability\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory,\n",
        "            \"cached_nsp_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "\n",
        "        # Input file format:\n",
        "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "        # sentence boundaries for the \"next sentence prediction\" task).\n",
        "        # (2) Blank lines between documents. Document boundaries are needed so\n",
        "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "        #\n",
        "        # Example:\n",
        "        # I am very happy.\n",
        "        # Here is the second sentence.\n",
        "        #\n",
        "        # A new document.\n",
        "\n",
        "        with FileLock(lock_path):\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "            else: # 캐시가 없는 경우\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "\n",
        "                self.documents = [[]] # document 단위로 학습이 이뤄짐\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    while True: \n",
        "                        line = f.readline() # 한줄씩 개행된 문장 \n",
        "                        if not line:\n",
        "                            break\n",
        "                        line = line.strip()\n",
        "\n",
        "                        # 이중 개행일 시, documents에 새로 document 추가\n",
        "                        if not line and len(self.documents[-1]) != 0:\n",
        "                            self.documents.append([])\n",
        "\n",
        "                        # line 별로 document에 추가\n",
        "                        tokens = tokenizer.tokenize(line)\n",
        "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                        if tokens:\n",
        "                            self.documents[-1].append(tokens)\n",
        "\n",
        "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
        "                self.examples = []\n",
        "\n",
        "                for doc_index, document in enumerate(self.documents):\n",
        "                    self.create_examples_from_document(document, doc_index) \n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
        "        \"\"\"Creates examples for a single document.\"\"\"\n",
        "        \n",
        "        # 총 입력 길이를 block_size로 지정했지만, \n",
        "        # Tokenizing 과정에서 [CLS], 입력1 tokens,[SEP] ,입력2 tokens,[SEP]형태로 들어가므로 3만큼 빼줘야함.\n",
        "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
        "\n",
        "        # We *usually* want to fill up the entire sequence since we are padding\n",
        "        # to `block_size` anyways, so short sequences are generally wasted\n",
        "        # computation. However, we *sometimes*\n",
        "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
        "        # The `target_seq_length` is just a rough target however, whereas\n",
        "        # `block_size` is a hard limit.\n",
        "\n",
        "        # 기본적으로 입력 데이터는 max_num_tokens를 꽉 채우는 형식으로 만들어짐\n",
        "        # 하지만 실제 입력 데이터는 max_len보다 짧은 데이터가 들어올 수 있음\n",
        "        # positioin embedding 등도 고려한다면 이런 부분에서 짧은 길이의 데이터도 넣어주는 것이 학습에 좋음\n",
        "        # 그래서 short_seq_probability 만큼의 데이터에서는 2 ~최대길이 사이의 random 값으로 짧은 길이의 데이터도 생성\n",
        "        target_seq_length = max_num_tokens\n",
        "        if random.random() < self.short_seq_probability:\n",
        "            target_seq_length = random.randint(2, max_num_tokens)\n",
        "\n",
        "        current_chunk = []  # a buffer stored current working segments\n",
        "        current_length = 0\n",
        "        i = 0\n",
        "\n",
        "        # document 단위로 데이터 생성\n",
        "        # 위에서 정한 target_seq_length을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 생성\n",
        "        while i < len(document):\n",
        "            segment = document[i]\n",
        "            current_chunk.append(segment)\n",
        "            current_length += len(segment)\n",
        "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "                if current_chunk:\n",
        "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "                    # (first) sentence.\n",
        "                    a_end = 1\n",
        "\n",
        "                    if len(current_chunk) >= 2:\n",
        "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
        "                    tokens_a = []\n",
        "                    for j in range(a_end):\n",
        "                        tokens_a.extend(current_chunk[j])\n",
        "\n",
        "                    tokens_b = []\n",
        "                    # 50%의 확률로 다음 문장을 이어서 넣거나, 다른 문서의 내용을 넣음 \n",
        "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
        "                        is_random_next = True\n",
        "                        target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "                        # This should rarely go for more than one iteration for large\n",
        "                        # corpora. However, just to be careful, we try to make sure that\n",
        "                        # the random document is not the same as the document\n",
        "                        # we're processing.\n",
        "                        for _ in range(10):\n",
        "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
        "                            if random_document_index != doc_index:\n",
        "                                break\n",
        "\n",
        "                        random_document = self.documents[random_document_index]\n",
        "                        random_start = random.randint(0, len(random_document) - 1)\n",
        "                        for j in range(random_start, len(random_document)):\n",
        "                            tokens_b.extend(random_document[j])\n",
        "                            if len(tokens_b) >= target_b_length:\n",
        "                                break\n",
        "                        # We didn't actually use these segments so we \"put them back\" so\n",
        "                        # they don't go to waste.\n",
        "                        num_unused_segments = len(current_chunk) - a_end\n",
        "                        i -= num_unused_segments\n",
        "                    # Actual next\n",
        "                    else:\n",
        "                        is_random_next = False\n",
        "                        for j in range(a_end, len(current_chunk)):\n",
        "                            tokens_b.extend(current_chunk[j])\n",
        "\n",
        "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
        "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "                        while True:\n",
        "                            total_length = len(tokens_a) + len(tokens_b)\n",
        "                            if total_length <= max_num_tokens:\n",
        "                                break\n",
        "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "                            assert len(trunc_tokens) >= 1\n",
        "                            # We want to sometimes truncate from the front and sometimes from the\n",
        "                            # back to add more randomness and avoid biases.\n",
        "                            if random.random() < 0.5:\n",
        "                                del trunc_tokens[0]\n",
        "                            else:\n",
        "                                trunc_tokens.pop()\n",
        "\n",
        "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
        "\n",
        "                    assert len(tokens_a) >= 1\n",
        "                    assert len(tokens_b) >= 1\n",
        "\n",
        "                    # add special tokens\n",
        "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
        "                    input_ids += [tokenizer.pad_token_id] * (self.block_size - len(input_ids))\n",
        "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
        "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
        "                    token_type_ids += [tokenizer.pad_token_id] * (self.block_size - len(token_type_ids))\n",
        "\n",
        "                    attention_mask = ([1] * len(input_ids)) + ([0] * (self.block_size - len(input_ids)))\n",
        "                    \n",
        "                    example = {\n",
        "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "                        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
        "                    }\n",
        "\n",
        "                    self.examples.append(example)\n",
        "\n",
        "                current_chunk = []\n",
        "                current_length = 0\n",
        "\n",
        "            i += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.examples[i]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvlBlEmF52ed"
      },
      "source": [
        "dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/my_data/wiki_20190620_small.txt',\n",
        "    block_size=model_max_input_len,\n",
        "    overwrite_cache=False,\n",
        "    short_seq_probability=0.1,\n",
        "    nsp_probability=0.5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(    # MLM의 masking 작업을 해주는 기능\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "kM78Q4zDWTE_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqJ3Av6KWnkK",
        "outputId": "11730fba-1207-4779-c8fd-251549c5059b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.examples[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tnhVyDKS3tz",
        "outputId": "37c80b0f-b598-4592-95e0-48061dba7093"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 input_ids에 masking 처리가 되고, labels가 생긴 것을 확인할 수 있음\n",
        "\n",
        "data_collator(dataset.examples).keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFKGjmaV3LbR",
        "outputId": "a0e124a2-47c9-4bcb-84ff-aa3f01978a8c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'next_sentence_label', 'labels'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(dataset.examples[1]['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "u_K1d0pFipa_",
        "outputId": "d461519d-1a2f-4a08-d698-9f9e49e616c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 퇴임 이후 민간 자원을 적극 활용한 비영리 기구인 카터 재단을 설립한 뒤 민주주의 실현을 위해 제 3세계의 선거 감시 활동 및 기니 벌레에 의한 드라쿤쿠르스 질병 방재를 위해 힘썼다. 미국의 빈곤층 지원 활동, 사랑의 집짓기 운동, 국제 분쟁 중재 등의 활동도 했다. 카터는 카터 행정부 이후 미국이 북핵 위기, 코소보 전쟁, 이라크 전쟁과 같이 미국이 군사적 행동을 최후로 선택하는 전통적 사고를 버리고 군사적 행동을 선행하는 행위에 대해 깊은 유감을 표시 하며 미국의 군사적 활동에 강한 반대 입장을 보이고 있다. 특히 국제 분쟁 조정을 위해 북한의 김일성, 아이티의 세드라스 장군, 팔레인스타인의 하마스, 보스니아의 세르비아계 정권 같이 미국 정부에 대해 협상을 거부하면서 사태의 위기를 초래한 인물 및 단체를 직접 만나 분쟁의 원인을 근본적으로 해결하기 위해 힘썼다. 이 과정에서 미국 행정부와 갈등을 보이기도 했지만, 전직 대통령의 권한과 재야 유명 인사들의 활약으로 해결해 나갔다. 1978년에 채결된 캠프데이비드 협정의 이행이 지지부진 하자 중동 분쟁 분제를 해결하기 위해 1993년 퇴임 후 직접 이스라엘과 팔레인스타인의 오슬로 협정을 이끌어 내는 데도 성공했다. [SEP] 1993년 1차 북핵 위기 당시 북한에 대한 미국의 군사적 행동이 임박했으나, 미국 전직 대통령으로는 처음으로 북한을 방문하고 미국과 북 양국의 중재에 큰 기여를 해 위기를 해결했다는 평가를 받았다. 또한 이 때 김영삼 대통령과 김일성 주석의 만남을 주선했다. 하지만 그로부터 수주일 후 김일성이 갑자기 사망하여 김일성과 김영삼의 정상회담은 이루어지지 못했다. 미국의 관타나모 수용소 문제, 세계의 인권문제에서도 관심이 깊어 유엔에 유엔인권고등판무관의 제도를 시행하도록 노력하여 독재자들의 인권 유린에 대해 제약을 하고, 국제형사재판소를 만드는 데 기여하여 독재자들 같은 인권유린범죄자를 재판소로 회부하여 국제적인 처벌을 받게 하는 등 인권 신장에 크나 큰 기여를 했다. 2011년 4월 26일부터 29일까지 북한을 3일간 방문했다. 경제문제를 해결하지 못하고 주 이란 미국 대사관 인질 사건에 발목이 잡혀 실패한 대통령으로 평가를 받지만 이란 사태는 미국 내 이란 재산을 풀어주겠다는 조건을 내세워서 사실상 카터가 해결한 것이었고, 사랑의 집짓기 운동 등으로 퇴임 후에 훨씬 더 존경받는 미국 대통령 중에 특이한 인물로 남았다. 그는 2002년 말 인권과 중재 역할에 대한 공로를 인정받아 노벨 평화상을 받게 되었다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "GMMUbKr_WvhG",
        "outputId": "e930578f-d74e-49b8-d335-0c56441bc517"
      },
      "source": [
        "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 제임스 얼 \" 지미 \" 카터 주니어는 민주당 [MASK] 미국 39번째 대통령 이다. 지미 카터는 조지아주 섬터 카운티 플레인스 [MASK]서 태어났다. 조지아 [MASK]를 졸업하였다. [MASK] 후 해군 [MASK] 들어가 전함 · 원자력 · 잠수함의 승무원으로 일하였다 [MASK] 1953년 미국 해군 대위로 예편 [MASK] 이후 땅콩 · 면화 [MASK] [UNK] [MASK] 돈을 벌었다. 그의 별 [MASK] \" 땅콩 농부 \" [MASK] 알려졌다. 1962년 조지아 주 상원 [MASK] 선거에서 낙선하나 그 선거가 부정선거 였 [MASK] 입증 [MASK] 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전화시켜 [MASK] [MASK] 두번 연임했으며, 1971년부터 [MASK] 조지아 [MASK] [MASK] [MASK]했다. 조지아 주지사로 지내면서, 미국에 [MASK] 흑인 등용법을 [MASK]. [MASK] 대통령 [MASK] 민주당 후보로 [MASK] 도덕 [MASK] 정책으로 내세워, 포드를 누르고 당선되었다 [MASK] 카터 대통령은 [MASK] 영어의 촉구했으나 공화당 [MASK] 반대로 무산되었다. 협회 이집트와 [MASK]을 조정하여, 캠프 데이 [MASK]드에서 안와르 사다트 대통령과 [UNK] 베긴 수상과 함께 중동 평화를 위한 캠프데이비드 협정을 [MASK]했다. [SEP] [MASK] 1979년 백악관에서 [MASK] 간의 평화조약으로 이끌어졌다. 또한 소련과 제2차 전략 무기 제한 협상에 조인했다. 카터는 1970년대 후반 [MASK] 대한민국 등 인권 후진국의 국민들의 인권을 지키기 위해 노력했으며, 취임 이후 계속해서 도덕정치 [MASK] 내세웠다. 그러나 주 이란 [MASK] 대사관 인질 사건에서 인질 구출 실패를 이유로 1980년 대통령 선거에서 공화당의 [MASK]널드 레이건 후보에게 져 결국 재선에 실패했다 [MASK] [MASK] 임기 말기에 터진 소련의 아프가니스탄 침공 사건으로 인해 1980년 하계 올림픽에 반공국가들의 [UNK] [MASK]. 지미 카터는 대한민국과의 관계에 [MASK] 중요한 영향을 미쳤던 대통령 중 하나다. 인권 [MASK] [MASK] 철수 문제로 한때 한미 [MASK] 불편하기도 했다. 1978년 대한민국에 대한 북한의 위협에 [MASK]해 한미연합사를 창설하면서, [MASK] [MASK] 3단 [MASK] 걸쳐 주한미군을 철수 [MASK] 했다. 그러나 주한미군 [MASK]부와 정보기관 · [MASK] 반대에 부딪혀 [MASK] [MASK] 완전철인에게 대신 [MASK], 000명을 [MASK]하는 데 [MASK] [MASK]. [MASK] 박정희 [MASK] 인권 [MASK] 등과의 [MASK] [MASK]협화음을 냈으나, 1979년 6월 하순, 대한민국을 방문하여 관계가 다소 회복되었다. 논할 ~ 1980년 대한민국의 정치적 격변기 당시의 [MASK]었던 그는 이에 [MASK] 애매한 태도를 보였고, 이는 [MASK] [MASK]부총리 고조되는 반미 운동의 한 원인이 됐다. 10월 26일, 박정희 대통령이 김재규 중앙정보부장에 가스는 살해된 것에 대해 그는 이 사건으로 큰 충격을 받았으며, 사이러스 밴스 국무장관을 조문사절로 핵했다 [MASK] 12 · 12 군사 반란과 [MASK]. 17 쿠데타에 대해 초기에는 [MASK] 비난했으나, 미국 정부가 신군부를 설득하는데 [MASK] 한계가 있었고 결국 [MASK] [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2o_hNfCZ8gjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.examples[1]['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XQhN3x08gl0",
        "outputId": "5d3f9c2b-1620-44dd-d05b-6a693a570266"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    2,  4277,  1935,  4097,  5825,  4400, 10178,  7946,  8030,  1040,\n",
            "         5506,  2823,  1096,  7068,   307,  4065,  5463,  1096,  1964,   733,\n",
            "         7193,  2806,  2328,  7219,  2195,   467,   182,  1252,   484,  1197,\n",
            "         1014,  2931, 13064,  1820,  1369,  6925,  8301,  9210,  1071,  1964,\n",
            "        16703,    17,  2506,   519,  1702,  1418,  2291,  2195,    15, 16210,\n",
            "        19161,  1944,    15,  2363,  6168, 11118,  2029,  2195,  1082,  2032,\n",
            "           17, 10314,  5506,  5237,  1935,  8589,  9244,  8229,    15, 19329,\n",
            "         1041,  2323,    15,  7859, 15090,  1991,  8589,  7812,  6507, 13948,\n",
            "         1036, 15024,  3001,  1008,  4233,  1071, 17970,  7812,  6507,  7345,\n",
            "         1898, 19585,  1949,  4216, 18792,  2936,  2617,  2506,  7812,  5637,\n",
            "         3135,  2703,  4750,  6688,  1889,    17,  2244,  2363,  6168, 13833,\n",
            "         1964,  7836, 16394,    15,  2545,  1031,  1007,   556, 11464,  1034,\n",
            "          721,  1358,    15, 19446, 15382,   936,  3367,    15,  5772,  2585,\n",
            "         1007,   556,  1182,  5571,  1144,  2452,  1991,  1970,  5619,  1949,\n",
            "         7523,  4517,  2141,  4717,  1007,  7408,  6816,  1012,  3406,   467,\n",
            "         2922,  1071,  2411,  3879,  6168,  1007, 13650,  5150,  1913,  7092,\n",
            "         1964, 16703,    17,   704,  3015,  1970,  5237,  1026, 10229, 18026,\n",
            "         5858,    15,  4401,  2669,  4522,  1098,  7432,  2529,  3234,  1971,\n",
            "         5860,  1887,  2709,  1167, 12941,    17,  7006,  1014,   783,  1454,\n",
            "         1259,  6824,  5901,  1186,  1200, 19607,  1007, 18834,  1009,  2217,\n",
            "        19963,  2660,  8293,  6168,   507,  2570,  7092,  1964,  3432,  4277,\n",
            "          984,  2411, 12617,  1098, 19446, 15382,   658, 11557,  8364,  5121,\n",
            "        10601, 17584,  2544,  1901,    17,     3,  3432,  3767,  9244,  8229,\n",
            "         2028, 10136,  1905,  2506,  7812, 10140, 18899,  2761,    15,  1970,\n",
            "         4401,  4457,  1016,  2856, 15487, 12013,  5300,   506, 13540, 11118,\n",
            "         1014,   854,  6522,   943,  7408,  2709,  2884,  4862,  2364,    17,\n",
            "         1928,   704,   326,  3529,  8570, 16394,  8290,  1007, 10686,  1096,\n",
            "        19080,  1901,    17,  2123, 17279, 10859,  1033,   984, 16394,  1009,\n",
            "        10502,  2385,  1902, 16394,  1098, 15746,  6506,  1054, 15185,  3393,\n",
            "           17,  2506,   157, 14446,  1377,  4237,  1068,  2004,    15,  6402,\n",
            "         3235,  5257,  2303,  6026, 17348,  4002,  1014,  4002, 14218,  6868,\n",
            "         1191,  1245,  9686, 11100,  4725,  3815,  3777,  1902,  4936,  3415,\n",
            "         3235,  7411,  1014,  1949,  9433,  2293,    15,  2363,  1231,  1044,\n",
            "         4162,  2785,  3335,   287,  3455,  1902,  4936,  5244,  1922,  3235,\n",
            "         1024,  1281, 14605,  2129,  3800,  1068,  1036,   979,  1077,  1902,\n",
            "         9984, 16284,  3783,  2016,   318,  3235, 13467, 11188,   854,  6522,\n",
            "         2032,    17,  3169,  2362,  5110,  1918,  4494,  1930, 15487,  4903,\n",
            "         1110,  7815,    17,  2198, 14323, 12009,  3620,   749,  4121,  1970,\n",
            "        12455, 13710,  6410, 17929,  1009,  8263,  5351,  4457,  4862,  4951,\n",
            "         1049,  4121,  4717,  1016,  1970,   220,  4121, 11067, 11227,  1021,\n",
            "         5931,  3963,  5734,  8530,  5092,  5506,  1047,  2709,  1012,  1990,\n",
            "         2687,    15, 16210, 19161,  1944,  2654,  4277,  3077,  4770,   279,\n",
            "        11107,  7613,  1970,  1942,  3350, 12535, 10212,   213,  1981,    17,\n",
            "         2006,  2767,   417,  3235,  1098, 11118, 11959,  1905, 17168,  1071,\n",
            "         8754,  1043,  2457, 10151,  3783,  1992,    17,     3,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "enJ5rYTB8goq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7u_9R16d8grS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpeY7jjMWsTW",
        "outputId": "da86899e-40a7-43fc-b220-5e04c4829406"
      },
      "source": [
        "print(data_collator(dataset.examples))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    2,  4356,   638,  ...,  2433,     4,     3],\n",
            "        [    2,  4277,  1935,  ...,     0,     0,     0],\n",
            "        [    2,  2001,   699,  ...,   280,  5859,     3],\n",
            "        ...,\n",
            "        [    2, 14532,  1932,  ...,     4,    17,     3],\n",
            "        [    2,    44,  3160,  ...,     0,     0,     0],\n",
            "        [    2, 16182,  2635,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]]), 'next_sentence_label': tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,   453,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        [ -100,  -100,  -100,  ..., 15369,  -100,  -100],\n",
            "        ...,\n",
            "        [ -100,  -100,  -100,  ...,    34,  -100,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n",
            "        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Ccr_cSXx2Whi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH7JzbMv5xBt",
        "outputId": "9d3bfa7a-0402-4f32-e9a3-b598f6b15042"
      },
      "source": [
        "config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n",
        "    vocab_size=tokenizer.vocab_size, \n",
        "    # hidden_size=512,\n",
        "    # num_hidden_layers=12,    # layer num\n",
        "    # num_attention_heads=8,    # transformer attention head number\n",
        "    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n",
        "    # hidden_act=\"gelu\",\n",
        "    # hidden_dropout_prob=0.1,\n",
        "    # attention_probs_dropout_prob=0.1,\n",
        "    max_position_embeddings=model_max_input_len,    # 해당 모델에서 사용할 수 있는 최대 입력 길이\n",
        "    # type_vocab_size=2,    # token type ids의 값 수 (BERT는 segmentA(0), segmentB(1))\n",
        "    # pad_token_id=0,\n",
        "    # position_embedding_type=\"absolute\"\n",
        ")\n",
        "\n",
        "model = BertForPreTraining(config=config)\n",
        "model.num_parameters()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102015010"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 1e-2\n",
        "early_stopping_patience = 10\n",
        "\n",
        "save_name = 'bert_pretraining'"
      ],
      "metadata": {
        "id": "W5oOfsnYGREZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "1eKDMH_DGVlD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvUWuEFG1-ln",
        "outputId": "40acd5fb-6dd7-4f9f-f3c3-f4ca557ba3e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTraining(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(20000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertPreTrainingHeads(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=20000, bias=True)\n",
              "    )\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class color:\n",
        "PURPLE = '\\033[95m'\n",
        "CYAN = '\\033[96m'\n",
        "DARKCYAN = '\\033[36m'\n",
        "BLUE = '\\033[94m'\n",
        "GREEN = '\\033[92m'\n",
        "YELLOW = '\\033[93m'\n",
        "RED = '\\033[91m'\n",
        "BOLD = '\\033[1m'\n",
        "UNDERLINE = '\\033[4m'\n",
        "END = '\\033[0m'"
      ],
      "metadata": {
        "id": "rbYdUjDD1-o-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8Fc4_rDRGoVD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(batch, epoch, training):\n",
        "    batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "    if training is True:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            loss = model(**batch)[0]\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        return loss, round(lr, 10)\n",
        "\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            loss = model(**batch)[0]\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "feuYt6CH1-c_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# train\n",
        "\n",
        "loss_plot = []\n",
        "lrs = []\n",
        "\n",
        "check_list = []\n",
        "\n",
        "best_loss = 100\n",
        "\n",
        "best_epoch = 0\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gc.collect()\n",
        "    total_loss, total_val_loss = 0, 0\n",
        "    \n",
        "    tqdm_dataset = tqdm(enumerate(train_dataloader), total=train_dataloader.__len__())\n",
        "    training = True\n",
        "    for batch_idx, batch in tqdm_dataset:\n",
        "        batch_loss, lr = train_step(batch, epoch, training)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        tqdm_dataset.set_postfix({\n",
        "            '%+10s' % 'Epoch': epoch + 1,\n",
        "            '%10s' % GREEN + 'Loss' : '{:.4f}'.format(total_loss/(batch_idx+1)) + END,\n",
        "            '%5s' % 'LR' : lr,\n",
        "        })\n",
        "            \n",
        "    loss_plot.append(total_loss/(batch_idx+1))\n",
        "    \n",
        "    cur_loss = round(float((total_loss/(batch_idx+1)).detach().cpu()), 3)\n",
        "\n",
        "    if cur_loss < best_loss:\n",
        "        print(YELLOW + 'Best_loss is updated from {:>5} to {:>5} on epoch {}'.format(best_loss, cur_loss, epoch+1) + END)\n",
        "        best_loss = cur_loss\n",
        "        best_epoch = epoch+1\n",
        "        torch.save(model.state_dict(), './'+save_name+'.ckpt')\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "    \n",
        "    lrs.append(lr)\n",
        "    \n",
        "    if patience == early_stopping_patience:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKJr6Qoz1-r3",
        "outputId": "3466aedb-4536-4c7e-8bdf-21482d5d8ce2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:58<00:00,  1.83it/s,      Epoch=1,      \u001b[92mLoss=9.6608\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from   100 to 9.661 on epoch 1\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=2,      \u001b[92mLoss=9.4877\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.661 to 9.488 on epoch 2\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=3,      \u001b[92mLoss=9.3538\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.488 to 9.354 on epoch 3\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=4,      \u001b[92mLoss=9.2576\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.354 to 9.258 on epoch 4\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=5,      \u001b[92mLoss=9.1823\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.258 to 9.182 on epoch 5\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=6,      \u001b[92mLoss=9.1337\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.182 to 9.134 on epoch 6\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=7,      \u001b[92mLoss=9.1070\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.134 to 9.107 on epoch 7\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=8,      \u001b[92mLoss=9.0561\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.107 to 9.056 on epoch 8\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=9,      \u001b[92mLoss=9.0275\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.056 to 9.028 on epoch 9\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=10,      \u001b[92mLoss=9.0019\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.028 to 9.002 on epoch 10\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=11,      \u001b[92mLoss=8.9753\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 9.002 to 8.975 on epoch 11\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=12,      \u001b[92mLoss=8.9339\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.975 to 8.934 on epoch 12\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=13,      \u001b[92mLoss=8.9024\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.934 to 8.902 on epoch 13\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=14,      \u001b[92mLoss=8.8953\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.902 to 8.895 on epoch 14\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=15,      \u001b[92mLoss=8.8450\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.895 to 8.845 on epoch 15\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=16,      \u001b[92mLoss=8.7954\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.845 to 8.795 on epoch 16\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=17,      \u001b[92mLoss=8.7489\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.795 to 8.749 on epoch 17\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=18,      \u001b[92mLoss=8.6981\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.749 to 8.698 on epoch 18\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=19,      \u001b[92mLoss=8.6308\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.698 to 8.631 on epoch 19\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=20,      \u001b[92mLoss=8.5696\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.631 to  8.57 on epoch 20\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=21,      \u001b[92mLoss=8.5020\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  8.57 to 8.502 on epoch 21\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=22,      \u001b[92mLoss=8.4556\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.502 to 8.456 on epoch 22\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=23,      \u001b[92mLoss=8.3805\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.456 to  8.38 on epoch 23\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=24,      \u001b[92mLoss=8.3606\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  8.38 to 8.361 on epoch 24\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=25,      \u001b[92mLoss=8.3480\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.361 to 8.348 on epoch 25\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=26,      \u001b[92mLoss=8.3238\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.348 to 8.324 on epoch 26\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=27,      \u001b[92mLoss=8.2654\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.324 to 8.265 on epoch 27\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=28,      \u001b[92mLoss=8.2303\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.265 to  8.23 on epoch 28\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=29,      \u001b[92mLoss=8.2468\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=30,      \u001b[92mLoss=8.2126\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  8.23 to 8.213 on epoch 30\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=31,      \u001b[92mLoss=8.2092\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.213 to 8.209 on epoch 31\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=32,      \u001b[92mLoss=8.1775\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.209 to 8.177 on epoch 32\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=33,      \u001b[92mLoss=8.1856\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=34,      \u001b[92mLoss=8.1744\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.177 to 8.174 on epoch 34\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=35,      \u001b[92mLoss=8.1316\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.174 to 8.132 on epoch 35\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=36,      \u001b[92mLoss=8.0649\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.132 to 8.065 on epoch 36\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=37,      \u001b[92mLoss=8.1114\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=38,      \u001b[92mLoss=8.0633\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.065 to 8.063 on epoch 38\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=39,      \u001b[92mLoss=8.0942\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=40,      \u001b[92mLoss=8.0857\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=41,      \u001b[92mLoss=8.0232\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.063 to 8.023 on epoch 41\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=42,      \u001b[92mLoss=8.0505\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=43,      \u001b[92mLoss=8.0386\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=44,      \u001b[92mLoss=8.0829\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=45,      \u001b[92mLoss=8.0383\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=46,      \u001b[92mLoss=7.9928\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 8.023 to 7.993 on epoch 46\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=47,      \u001b[92mLoss=7.9987\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=48,      \u001b[92mLoss=7.9382\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.993 to 7.938 on epoch 48\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=49,      \u001b[92mLoss=8.0105\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=50,      \u001b[92mLoss=7.9555\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=51,      \u001b[92mLoss=7.9633\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=52,      \u001b[92mLoss=7.9720\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=53,      \u001b[92mLoss=7.9694\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=54,      \u001b[92mLoss=7.9304\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.938 to  7.93 on epoch 54\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=55,      \u001b[92mLoss=7.9425\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=56,      \u001b[92mLoss=7.9200\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  7.93 to  7.92 on epoch 56\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=57,      \u001b[92mLoss=7.9265\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=58,      \u001b[92mLoss=7.9439\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=59,      \u001b[92mLoss=7.9456\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=60,      \u001b[92mLoss=7.9157\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  7.92 to 7.916 on epoch 60\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=61,      \u001b[92mLoss=7.9129\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.916 to 7.913 on epoch 61\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=62,      \u001b[92mLoss=7.8871\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.913 to 7.887 on epoch 62\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=63,      \u001b[92mLoss=7.8575\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.887 to 7.857 on epoch 63\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=64,      \u001b[92mLoss=7.8564\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.857 to 7.856 on epoch 64\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=65,      \u001b[92mLoss=7.9109\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=66,      \u001b[92mLoss=7.8537\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.856 to 7.854 on epoch 66\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=67,      \u001b[92mLoss=7.8600\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=68,      \u001b[92mLoss=7.8321\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.854 to 7.832 on epoch 68\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=69,      \u001b[92mLoss=7.8307\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.832 to 7.831 on epoch 69\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=70,      \u001b[92mLoss=7.8546\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=71,      \u001b[92mLoss=7.8203\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.831 to  7.82 on epoch 71\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=72,      \u001b[92mLoss=7.8452\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=73,      \u001b[92mLoss=7.7866\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  7.82 to 7.787 on epoch 73\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=74,      \u001b[92mLoss=7.7827\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.787 to 7.783 on epoch 74\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=75,      \u001b[92mLoss=7.7835\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=76,      \u001b[92mLoss=7.7690\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.783 to 7.769 on epoch 76\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=77,      \u001b[92mLoss=7.7777\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=78,      \u001b[92mLoss=7.7706\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=79,      \u001b[92mLoss=7.7822\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=80,      \u001b[92mLoss=7.7687\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=81,      \u001b[92mLoss=7.7536\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.769 to 7.754 on epoch 81\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=82,      \u001b[92mLoss=7.7232\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.754 to 7.723 on epoch 82\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=83,      \u001b[92mLoss=7.7174\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.723 to 7.717 on epoch 83\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=84,      \u001b[92mLoss=7.7569\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=85,      \u001b[92mLoss=7.7289\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=86,      \u001b[92mLoss=7.7006\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.717 to 7.701 on epoch 86\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=87,      \u001b[92mLoss=7.7136\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=88,      \u001b[92mLoss=7.6902\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.701 to  7.69 on epoch 88\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=89,      \u001b[92mLoss=7.7058\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=90,      \u001b[92mLoss=7.6810\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from  7.69 to 7.681 on epoch 90\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=91,      \u001b[92mLoss=7.6982\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=92,      \u001b[92mLoss=7.7076\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=93,      \u001b[92mLoss=7.6761\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.681 to 7.676 on epoch 93\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=94,      \u001b[92mLoss=7.6593\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.676 to 7.659 on epoch 94\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=95,      \u001b[92mLoss=7.6110\u001b[0m,    LR=1e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mBest_loss is updated from 7.659 to 7.611 on epoch 95\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=96,      \u001b[92mLoss=7.6331\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=97,      \u001b[92mLoss=7.6344\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.84it/s,      Epoch=98,      \u001b[92mLoss=7.6257\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=99,      \u001b[92mLoss=7.6513\u001b[0m,    LR=1e-5]\n",
            "100%|██████████| 106/106 [00:57<00:00,  1.83it/s,      Epoch=100,      \u001b[92mLoss=7.6222\u001b[0m,    LR=1e-5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1h 15min 58s, sys: 21min 23s, total: 1h 37min 22s\n",
            "Wall time: 1h 37min 49s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./bert_model')"
      ],
      "metadata": {
        "id": "BRvTyFk6fHpm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtuDt--Yb1Bx"
      },
      "source": [
        "## Filling Mask Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCg6A9rMVsIY",
        "outputId": "4816913f-5c97-450a-c892-5497cb8b95e5"
      },
      "source": [
        "my_model = BertForMaskedLM.from_pretrained('./bert_model')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./bert_model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e1pqEqKWgGT",
        "outputId": "0533d996-b18c-4e69-cbcf-3bee3959b0b5"
      },
      "source": [
        "tokenizer.tokenize('이순신은 [MASK] 중기의 무신이다.')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVYg7-PmVzvH"
      },
      "source": [
        "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPn9azZoV7aC",
        "outputId": "931d37d6-aa2f-42f3-91cf-1562c5b791c3"
      },
      "source": [
        "nlp_fill('이순신은 [MASK] 중기의 무신이다.')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.0300902146846056,\n",
              "  'token': 1034,\n",
              "  'token_str': '##의',\n",
              "  'sequence': '[CLS] 이순신은의 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.01777038536965847,\n",
              "  'token': 17,\n",
              "  'token_str': '.',\n",
              "  'sequence': '[CLS] 이순신은. 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.015896081924438477,\n",
              "  'token': 1067,\n",
              "  'token_str': '##는',\n",
              "  'sequence': '[CLS] 이순신은는 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.006974676623940468,\n",
              "  'token': 705,\n",
              "  'token_str': '이',\n",
              "  'sequence': '[CLS] 이순신은 이 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.00679297000169754,\n",
              "  'token': 15,\n",
              "  'token_str': ',',\n",
              "  'sequence': '[CLS] 이순신은, 중기의 무신이다. [SEP]'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kYenPbpZqhP",
        "outputId": "2439c53a-1d7b-4388-c2e0-4e6306abf44d"
      },
      "source": [
        "nlp_fill('[MASK]는 조선 중기의 무신이다.')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.024894190952181816,\n",
              "  'token': 705,\n",
              "  'token_str': '이',\n",
              "  'sequence': '[CLS] 이 는 조선 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.014039567671716213,\n",
              "  'token': 17,\n",
              "  'token_str': '.',\n",
              "  'sequence': '[CLS]. 는 조선 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.006746664177626371,\n",
              "  'token': 175,\n",
              "  'token_str': '그',\n",
              "  'sequence': '[CLS] 그 는 조선 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.006324885878711939,\n",
              "  'token': 1925,\n",
              "  'token_str': '또한',\n",
              "  'sequence': '[CLS] 또한 는 조선 중기의 무신이다. [SEP]'},\n",
              " {'score': 0.0050242007710039616,\n",
              "  'token': 1034,\n",
              "  'token_str': '##의',\n",
              "  'sequence': '[CLS]의 는 조선 중기의 무신이다. [SEP]'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_ueooCTfWJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}